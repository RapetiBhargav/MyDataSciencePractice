Approach towards the problem:
=============================

1. Done EDA, data preprocessing and feature engineering(created extra features from DATE). 

2. In DataPreprocessing step,  I have imputed missing values of X_12 feature by respective X_10 since they are correlated.
   Removed INCIDENT_ID feature before training.
   
   Correlated features - [X2,X3]-0.99 and [X_10,X_12]-0.877

3. For a more modular approach , I have included each of the classifiers into definitions. I have included the Data-Preprocessing step in this. 
   We just need to call the definitions with train and test dataframes as arguments.

3. I have tried different models like KNN ,Decision Trees and Random Forrest initially. They did not give a good score.(Not more than 98.44 for Decision Trees)

4. Later I went on to try Adaboost and XGB. I got a score of 99.37 and 99.44 on submission.

5. Finally , since we have all categorical features, I tried my luck with CatBoost whose parameters are optimized with Scikit-Optimize.

6. I have evaluated the Catboost Model on entire Train dataset using Confusion matrix, and the diagnols of the matrix were close to 0 , which was good enough.

7. I got a score of 100 on submission for Catboost.

8. I have worked on Google Colab for this problem.