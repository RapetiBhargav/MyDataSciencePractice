5.From Novice to Master Predictor

Intuition for recognizing opportunities to improve your model’s accuracy.

We explore several questions that tend to come up during model training:

1.I want to ensure that the model is learning the correct thing
and not picking up spurious correlations. How can I get
visibility into that?

2. reproducibility of my experiments? Use as Seed. seeds are not transferable between frameworks.

3. Does changing the aspect ratio of the input images have an
impact on the predictions?

4. Does reducing input image size have a significant effect on
prediction results?

5. There are too many things to remember. Is there a way to
automate all of this work?

6. Effect of optimizers
Adam is a great choice for faster convergence to high accuracy.
RMSProp is usually better for RNN tasks.

Tools of the Trade:
===================
TensorFlow Datasets
TensorBoard:  Close to 20 easy-to-use methods to visualize many aspects of training, including visualizing the graph, tracking experiments,
			  and inspecting the images, text, and audio data that pass through the network during training.
			  You can see progression of training, compare training graphs.

			  TensorBoard is not TensorFlow specific,
			  and can be used with other frameworks like PyTorch, scikit-learn,
			  and more, depending on the plugin used.


What-If Tool: Run experiments in parallel on separate models and tease out
			  differences in them by comparing their performance on
			  specific data points.
			  Enables model and data explainability.
			  To use the What-If Tool, we need the dataset and a model - 
			  Save the model(.pb) and dataset(tfrecords) in a new directory. 
			  We’ll serve the model using Docker within the newly created directory.
			  and you need to give these details in the What-If settings page when you run TensorBoard.
			  
			  What-If tools - Datapoint editor, Performance And Fairness(PR, ROC curves),Visualize datasets as per different bins,
			  Use set_compare_estimator_and_feature_spec function to compare different models.
			  
tf-explain: Analyze decisions made by the network to identify bias and
			inaccuracies in the dataset. Additionally, use heatmaps to
			visualize what parts of the image the network activated on.
			
			We can add multiple types of callbacks while training or use its core API to
			generate TensorFlow events that can later be loaded into TensorBoard.
			
			For inference, all we need to do is pass an image, its	ImageNet object ID along with a model into tf-explain’s functions.
			You must supply the object ID because tf.explain needs to know what is activated for that particular class. 
			A few different visualization approaches are available with tf.explain.
			1.Gradient-weighted Class Activation Mapping (Grad CAM)
			2.Occlusion Sensitivity
			3.Activations
			
from tf_explain.core.grad_cam import GradCAM
explainer = GradCAM()
grid = explainer.explain(data, model, 'conv1', index) 		>>>> Passing the image thru GradCAM
			
Keras Tuner: For tf.keras, hyperparameters
			 Another big benefit is the ability to track experiments online in real
			 time and get notifications on their progress by visiting http://kerastuner.appspot.com, 
			 getting an API key (from Google App Engine),

AutoKeras: Automates NAS
           NAS approaches utilize reinforcement learning to join together mini-architectural
           blocks until they are able to maximize the objective function.

AutoAugment: Utilizes reinforcement learning to improve the amount and diversity of data in an existing training dataset, 
			 thereby increasing accuracy.
			 
https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/Checklist.md