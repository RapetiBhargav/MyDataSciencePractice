Deep Computer Vision Using CNNs
================================
They power image search services, self-driving cars, automatic video classification systems, and
more. Moreover, CNNs are not restricted to visual perception: they are also successful
at many other tasks, such as voice recognition and natural language processing. However,
we will focus on visual applications for now.

The best CNN architectures, as well as other visual tasks, including 
1. Object detection (classifying multiple objects in an image and placing bounding boxes around them) 
2. Semantic segmentation (classifying each pixel according to the class of the object it belongs to).

Why not DNNs with fully connected layers for image recognition tasks? 
Unfortunately, although this works fine for small images (e.g., MNIST), it breaks down for
larger images because of the huge number of parameters it requires.
Parameters burst - 1 mill for a layer of 100*100 , 1000 neurons dense layer

CNNs solve this problem using partially connected layers and weight sharing.

Outputs one feature map per filter. It has one neuron per pixel in each feature map, and all neurons
within a given feature map share the same parameters (i.e., the same weights and bias term)

The fact that all neurons in a feature map share the same parameters dramatically reduces the number of parameters in the model.
Once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a regular
DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location. >>> Spatial Invariance

Convolution Layers:
===================
This architecture allows the network to concentrate on small low-level features in the first hidden layer, 
then assemble them into larger higher-level features in the next hidden layer, and so on. 
This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.

Zero padding:
In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs.
padding must be either "SAME" or "VALID":

Filters: filters (or convolution kernels).

A layer full of neurons using the same filter outputs a feature map,

Horizontal filter, Vertical filter

Stacking Multiple Feature Maps:
==============================
Input images are also composed of multiple sublayers: RGB or GrayScale
But some images may have much more—for example, satellite images that capture extra light frequencies (such as infrared)

Check snapshot.

TensorFlow Implementation:
==========================
In TensorFlow, each input image is typically represented as a 3D tensor of shape [height, width, channels]. 
A mini-batch is represented as a 4D tensor of shape [minibatch size, height, width, channels].
The weights of a convolutional layer are represented as a 4D tensor of shape [fh, fw, fn′, fn]. fh, fw are dimensions of filter. fn′ are features maps in previous layer, fn is number of biases(repr. as a 1D tensor).

outputs = tf.nn.conv2d(images, filters, strides=1, padding="SAME")    >>>>>>>>>>>>TensorFlow’s low-level Deep Learning API.
images - 4D
filters - 4D

Instead of manually creating these filters use Conv2D

conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding="same", activation="relu")

Lot of Hyperparameters - filters, size, strides, padding etc...
As always, you can use cross-validation to find the right hyperparameter values, but this is very time-consuming. We will discuss common
CNN architectures later, to give you some idea of which hyperparameter values work best in practice.

Memory Requirements:
====================
Consider a convolutional layer with 5 × 5 filters, outputting 200 feature maps of size 150 × 100, with stride 1 and "same" padding. 
If the input is a 150 × 100 RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200
= 15,200 (the + 1 corresponds to the bias terms), which is fairly small compared to a fully connected layer. 

However, each of the 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully connected layer, but still quite computationally intensive. Moreover, if the feature maps are represented using 32-bit floats, then the convolutional layer’s output will occupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM. And that’s just for one instance—if a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!

During inference 
The RAM occupied by one layer can be released as soon as the next layer has been computed, so you
only need as much RAM as required by two consecutive layers.
But During training
Everything computed during the forward pass needs to be preserved for the reverse pass, 
so the amount of RAM needed is (at least) the total amount of RAM required by all layers.

If training crashes because of an out-of-memory error, you can try
reducing the mini-batch size. Alternatively, you can try reducing
dimensionality using a stride, or removing a few layers. Or you can
try using 16-bit floats instead of 32-bit floats. Or you could distribute
the CNN across multiple devices.

Pooling Layers:
===============
Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters
(thereby limiting the risk of overfitting).

max pooling layer, which is the most common type of pooling layer.

Other than reducing computations, memory usage, and the number of parameters, a
max pooling layer also introduces some level of invariance to small translations, 

pooling kernels (stateless sliding kernals- stateless means no weights)

Gives Translation invariance
In some applications, invariance is not desirable. Take semantic segmentation (the task of classifying each pixel in an image according to the
object that pixel belongs to). The goal in this case is equivariance, not invariance.
(a small change to the inputs should lead to a corresponding small change in the output.)

TensorFlow Implementation:
==========================
MaxPool2D
AvgPool2D
Depthwise pooling
Global Average Pooling layer

max pooling layers generally perform better.

max pooling and average pooling can be performed along the depth dimension rather than the spatial dimensions, although this is not as common. 
This can allow the CNN to learn to be invariant to various features.
The depthwise max pooling layer would ensure that the output is the same regardless of the rotation.
Depthwise max pooling can help the CNN learn any invariance.
It could learn multiple filters, each detecting a different rotation of the same pattern (such as handwritten digits)

Keras does not include a depthwise max pooling layer, but TensorFlow’s low-level Deep Learning API does: just use below

output = tf.nn.max_pool(images,
ksize=(1, 1, 1, 3),
strides=(1, 1, 1, 3),
padding="valid") 

If you want to include this as a layer in your Keras models, wrap it in a Lambda layer (or create a custom Keras layer):
depth_pool = keras.layers.Lambda( lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding="valid"))

global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))

CNN Architectures
=================
Typical CNN architecture
Instead of using a CNN with a 5 × 5, use 2 layers with 3 × 3 kernals.  One exception is for the first convolutional layer: it can typically
have a large kernel (e.g., 5 × 5), usually with a stride of 2 or more: this will reduce the spatial dimension of the image without
losing too much information, and since the input image only has three channels in general, it will not be too costly.

Double the number of filters after each pooling layer

Usually Convolution layers--->>Pooling Layers--->>Convolution--->>Pooling.......--->>Dense layers--->>Output layer

Top-five error rate:
===================
The top-five error rate is the number of test images for which the system’s top
five predictions did not include the correct answer.

LeNet
=====
Avg Pooling - tanh
RBF in the end

AlexNet
=======
It is similar to LeNet-5, only much larger and deeper.
Stack convolutional layers directly on top of one another, instead of stacking a
pooling layer on top of each convolutional layer.

Used DropOut + Performed Data Augmentation(increase the size of training set) + LRN ===>> both for Regularization


LRN:
====
AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called local response normalization (LRN): 
the most strongly activated neurons INHIBIT other neurons located at the same position in neighboring feature maps

This step can be implemented using the tf.nn.local_response_normalization() function 
(which you can wrap in a Lambda layer if you want to use it in a Keras model).

Hyperparameters:
r = 2(depth radius - inhibit the layer above and below), α = 0.00002, β = 0.75, and k = 1.

ZF Net -- A variant of AlexNet

GoogLeNet
=========
This great performance came in large part from the fact that the network was much deeper than previous CNNs.
Made possible by subnetworks called inception modules, which allow GoogLeNet to use parameters much more efficiently than previous architectures: 
GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).

Outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimension
in the final depth concatenation layer (i.e. tf.concat() operation, with axis=3 (the axis is the depth))

Usage of 1 x 1 
==============
1. Although they cannot capture spatial patterns, they can capture patterns along the depth dimension
2. They are configured to output fewer feature maps than their inputs, so they serve
   as bottleneck layers, meaning they reduce dimensionality. Hence fewer parameters, good training speed

Inception module Sweeps a two-layer neural network across the image.(CNN on steroids !! - Smarter Convolution Layer)

Number of convolution kernels is hyperparameter - So 6 hyperparameters per inception module.
224 × 224 to 7 × 7 due to 5 max-pooling layers.

We have a global average pooling layer at the end which outputs the mean of each feature map:instead of several fully connected layers at the top of the CNN.
This drops any remaining spatial information, which is fine because there was not much spatial information left at that point.

Sometimes few auxillary classifiers(avg pooling layer + convolutional layer + 2 FC , softmax) are also plugged in on top to some inception modules(To fight the vanishing gradient and regularize the network)

VGGNet
======
The Visual Geometry Group (VGG) research lab at Oxford University.
2,3 Conv layers + Max pool layers repeat.....+ 2 hidden + O/p layer

VGG-16 or VGG-19
16 or 19 . This is the number of weight layers in the architecture(i.e conv + Dense layers, no max pooling)

ResNet(34, 50, 101, 152)
======
It confirmed the general trend: models are getting deeper and deeper, with fewer and fewer parameters. 
The key to being able to train such a deep network is to use skip connections (also called shortcut connections): 
the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack. Let’s
see why this is useful.

Instead of h(x) , we learn f(x) = h(x) – x

Each residual unit is composed of 
2 convolutional layers with (BN + ReLU), for last one BN-->Skip Connection added-->ReLU

Note that the number of feature maps is doubled every few residual units, at the same
time as their height and width are halved (using a convolutional layer with stride 2).
When this happens, the inputs cannot be added directly to the outputs of the residual
unit because they don’t have the same shape. To solve this problem,
the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the
right number of output feature maps.

ResNet-34 :
1 conv 
3 RUs with 64  maps
4 RUs with 128 maps
6 RUs with 256 maps
3 RUs with 512 maps
1 dense

(3+4+6+3)*2 + 2=34

34 vs 152 
ResNet-34 Residual Unit : 2 3 × 3 256 maps 
ResNet-152 Residual Unit : 1 × 1 64 maps +  3 × 3 64 maps + 1 × 1 256 maps 		>>>>>>> 1 × 1 to restore the original depth

ResNet-152 :
(3+8+36+3)*3 + 2																>>>>>>> 3 per residual unit

Google’s Inception-v4 : Merged the ideas of GoogLeNet and ResNet

Xception(Extreme Inception)
========
Just like Inception-v4, it merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special type of layer
called a depthwise separable convolution layer (or separable convolution layer)

While a regular convolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and crosschannel
patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer makes the strong assumption that spatial patterns 
and cross-channel patterns can be modeled separately. 
Thus, it is composed of two parts: 
the first part applies a single spatial filter for each input feature map, then the second part looks
exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 × 1 filters.

1 × 1								Regular ones on top of it									Seperable Convolution layer
For cross channel patterns			Considers spatial and cross-channel patterns jointly		Considers spatial and cross-channel patterns seperately


The Xception architecture starts with 2 regular convolutional layers, next all Xception modules.(since you should avoid using them after layers that have too few channels, like input layer) 

keras.layers.SeparableConv2D or tf.layers.separable_conv2d.
1.MobileNet will use the concept of separable convolutions.
MobileNet is essentially a streamlined version of the Xception architecture optimized for mobile applications.
2.Parameters are reduced in separable convolutions compared to regular convolutions.
3.SeparableConolutions are divided into a)Depthwise Convolution(3 5*5*1 filters instead of 1 5*5*3) and b) Pointwise Convolution(1*1 to stretch the depth to desirable dimension)

How is this similar to Inception?

https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728

SENet
=====
This architecture extends existing architectures such as inception networks and ResNets, and boosts their performance.
The extended versions of inception networks and ResNets are called SE-Inception and SE-ResNet.Adds a SE block

An SE block analyzes the output of the unit it is attached to, focusing exclusively on
the depth dimension (it does not look for any spatial pattern), and it learns which features
are usually most active together. It then uses this information to recalibrate the
feature maps.
Eg: nose, mouth...lets boost eyes features.

SE Block=global avg(256 feature maps - like 256 neurons) + dense layer, ReLU(16 neurons - here is where the squeeze happens - Embeddings - Autoencoders) + dense layer, sigmoid(256 neurons - between 0 to 1 - The recalibration vector)

The recalibration vector * Actual vector = Recalibrated scaled output

Used in EfficientNet(as a SE block)

Implementation in Keras- Resnet34
========================
Generally you would load a pretrained network instead.
The skip layers are the ones on the left (only needed if the stride is greater than 1)

for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:

How to use Pretrained models.
=============================
Pretrained networks are readily available with a single line of code in the :: >>>keras.applications package.
load the ResNet-50 model, pretrained on ImageNet

A ResNet-50 model expects 224 × 224-pixel images >>>> images_resized = tf.image.resize(images, [224, 224]) or tf.image.crop_and_resize(). to maintain aspect ratio also

inputs = keras.applications.resnet50.preprocess_input(images_resized * 255) >>>>>Preprocess and pixel values must be between 0-255

Perform Data Augmentation : tf.image.random_crop(), tf.image.random_flip_left_right()
keras.preprocessing.image.ImageDataGenerator class can load, preprocess and augment (but tf.data pipeline, tf.image has many advantages in production)

After training for a few epochs, unfreeze
(don’t forget to compile the model when you freeze or unfreeze layers). This time we use a much lower learning
rate to avoid damaging the pretrained weights:

Classification and Localization
===============================
It does not require much change to the model; we just need to add a second dense output layer with four
units (typically on top of the global average pooling layer), and it can be trained using the MSE loss:

class_output = keras.layers.Dense(n_classes, activation="softmax")(avg)
loc_output = keras.layers.Dense(4)(avg)
model = keras.Model(inputs=base_model.input,
					outputs=[class_output, loc_output])
model.compile(loss=["sparse_categorical_crossentropy", "mse"],
			  loss_weights=[0.8, 0.2], # depends on what you care most about
			  optimizer=optimizer, metrics=["accuracy"])

To annotate images with bounding boxes, you may want to use an open source image labeling tool like VGG Image
Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a commercial tool like LabelBox or Supervisely. 
You may also want to consider crowdsourcing platforms such as Amazon Mechanical Turk.

Crowdsourcing in Computer Vision:
https://arxiv.org/pdf/1611.02145.pdf

The MSE often works fairly well as a cost function to train the model, but it is not a
great metric to evaluate how well the model can predict bounding boxes. The most
common metric for this is the Intersection over Union (IoU): tf.keras.metrics.MeanIoU class.

Object Detection
================
The task of classifying and localizing multiple objects in an image is called object detection.
Probable Solution: Take a CNN(each time of different size) which was trained on the object. Slide it on the image.
Problem: Many bounding boxes. Some post-processing will then be needed to get rid of all the unnecessary bounding boxes. 
A common approach for this is called non-max suppression.

1. Get rid of all the bounding boxes for which the objectness score is below some threshold.
2. Find the bounding box with the highest objectness score, and get rid of all the
   other bounding boxes that overlap a lot with it(IoU > 60%)

Use FCN(Fully convolutional Network)
====================================
This was introduced in a Semantic Segmentation paper.
Basic Idea: You could replace the dense layers at the top of a CNN by convolutional layers.

Replace 200 neuron dense layer with 200 filters, each of size 7 × 7(previous layer's feature map size), and with "valid" padding

Dense layer’s output was a tensor of shape     [batch size, 200]
Convolutional layer's output a tensor of shape [batch size, 1, 1, 200]

We can just copy the weights from the dense layers to the convolutional layers

You Only Look Once (YOLO)(Object Detection Architecture)
=========================
YOLOv3’s architecture:
1.Trained on Pascal VOC dataset.
So 20 classifications, 5 Bounding boxes per grid.....so output is 45 numbers(20+ 5 objectness score + 5 bounding * 4 coordinates)

2. YOLOv3 applies the logistic activation function to the bounding box coordinates to ensure they remain in the 0 to 1 range.
So Instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 predicts an offset relative to the coordinates of the grid cell.

3. Didnt understand

4. It is possible to use YOLOv3 at different scales: 
The smaller scale will be less accurate but faster than the larger scale, so you can
choose the right trade-off for your use case.

More innovations:
1. The use of skip connections to recover some of the spatial resolution that is lost in the CNN.
2. The YOLO9000 model that uses hierarchical classification: the model predicts a probability for each node in a visual hierarchy called WordTree.
Eg: Predict dog, even though it is unsure what specific type of dog

Mean Average Precision (mAP): - A very common metric for object detection.
============================
In a PR curve:
The precision/recall curve may contain a few sections where precision actually goes up
when recall increases, especially at low recall values.
This is one of the motivations for the mAP metric.

We should really be looking at the maximum precision that the classifier can offer with at least 0%,10%,20%..100% recall.
Then calculate the mean of these maximum precisions - Average Precision(AP)
If you have > 2 classes, compute Average Precisions for all of these and take Mean - mAP

How Positive or Negative prediction?? Based on certain IOU threshold.
So we have something like  
mAP@0.50,mAP@0.60 etc......
In some competitions you take mean of all these mAPs.....AP@[.50:0.05:.95])--->> That's a mean mean average

Other Object Detection models:
SSD - Similar to YOLO
Faster-RCNN : The image first goes through a CNN, then the output is passed to a Region Proposal Network (RPN) that proposes
bounding boxes that are most likely to contain an object.

Semantic Segmentation(classification at pixel level)
=====================
Different objects of the same class are not distinguished.(Need to use instance segmentation for that)

The main difficulty in this task is that when images go through a regular CNN, they gradually
lose their spatial resolution

The authors start by taking a pretrained CNN and turning it into an FCN. The CNN applies an overall stride of 32 to the input image (i.e., if you add up all the strides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than the input image. This is clearly too coarse, so they add a single upsampling layer that multiplies the resolution by 32.

Several solutions available for upsampling (increasing the size of an image),
such as bilinear interpolation, but that only works reasonably well up to ×4 or ×8.
Instead, they use a transposed convolutional layer.

This solution is OK, but still too imprecise. 
To do better, the authors added skip connections from lower layers: for example, they upsampled the output image by a factor
of 2 (instead of 32), and they added the output of a lower layer that had this double
resolution.
Basically adding the lower layers as skip connections to bring back the spacial resolution.(See pic)

Extras:
1.NPTEL video: https://www.youtube.com/watch?v=_N7HRnBgoCw
SegNet,UNet and DenseNet

2.Geoffrey Hinton’s capsule networks.

OpenCV:
========
https://www.youtube.com/watch?v=WQeoO7MI0Bs&t=63s

1. Read images videos and webcam
2. Basic OpenCV functions
3. Resizing and croping
4. Shapes and Text
5. Warp Perspective
6. Joining Images
7. Color Detection
8. Contour/Shape Detection
9. Face Detection

Projects:
Virtual Paint
Paper Scanner
Number plate detector

img=cv2.imread("path")
cv2.imshow()
cv2.waitKey(0)

cap=cv2.VideoCapture("path/*.mp4")
while True:
	success,img=cap.read()
	cv2.imshow()
	
imgGray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
imgBlur=cv2.GaussianBlur(imgGray,(7,7),0) - number should be odd Eg.3,5,7 
imgCanny=cv2.Canny(img,100,100) --------->>>Canny edge detector
imgDialation=cv2.dilate(imgCanny)

Coursera:
Face Recognition:
=================
Liveness Detection can be implemented using Supervised learning.

Face Verification(1:1) vs Face Recognition(1:K)
Focus on Building a Face Verification system as a building block..
If the accuracy is good you can use it for Face Recognition system as well.

One Shot Learning
=================
Why Face verification is difficult??
You need to build a One Shot Learning. i.e learn from just example(Think why its hard to train)

Learn a Similarity function(see pic)

Siamese Network
===============
Good way to calculate similarity is by using a Siamese NN
Siamese NN : Running 2 different inputs on the same network and comparing them.

So how do you train this Siamese NN?-See pic..For simiar images your goal is to maximize similarity , for different images minimize similarity
But you need to have an objective function to make it learn.
2 ways:
1. Triplet Loss
2. Binary Classification

Triplet Loss
============
Anchor,Positive,Negative
See Pic TripletLoss-Margin
See Pic TripletLoss-Function
Training data is Triplet of pictures.

****But how do you choose these Triplets to form your Training set???
Choose Triplets that are "Hard" to train on.
See Pic TripletLoss-ChoosingTripletsForTraining

Commercial Face Recognition systems are trained on 1-100million images
Rather than training one of these networks from scratch, download someone else's pretrained model.

Face Verification And Binary Classification
===========================================
See Pic FaceVerificationAsBinaryClassProblem
Training data is pairs of pictures.
Important: While inference you can just precompute the embeddings of the images in the Database...so that you can just compute the embedding from new image and compare them..You can do this for both Binary and Triplet Loss.

To Learn:
Neural Style Transfer
Content Cost function
1D and 3D generalizations
Focal loss,SSD

Some stuff from the Practical Deep Learning book:



RCNN,Fast-RCNN,Faster-RCNN:
==============================
