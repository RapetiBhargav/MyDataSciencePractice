{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Week 5 - Logistic Regression and Classification Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n",
    "\n",
    "Alternatively the same data set can be found at https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones/downloads/human-activity-recognition-with-smartphones.zip\n",
    "The train file can be renamed as Human_Activity_Recognition_Using_Smartphones_Data.csv \n",
    "\n",
    "For each record in the dataset it is provided: \n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. \n",
    "- Triaxial Angular velocity from the gyroscope. \n",
    "- A 561-feature vector with time and frequency domain variables. \n",
    "- Its activity label. \n",
    "\n",
    "More information about the features is available on the website above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "#Data Path has to be set as per the file location in your system\n",
    "#data_path = ['..', 'data']\n",
    "data_path = ['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Import the data and do the following:\n",
    "\n",
    "* Examine the data types--there are many columns, so it might be wise to use value counts\n",
    "* Determine if the floating point values need to be scaled\n",
    "* Determine the breakdown of each activity\n",
    "* Encode the activity label as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#The filepath is dependent on the data_path set in the previous cell \n",
    "filepath = os.sep.join(data_path + ['Human_Activity_Recognition_Using_Smartphones_Data.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 562)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data columns are all floats except for the activity label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
       "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
       "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
       "       'tBodyAcc-max()-X',\n",
       "       ...\n",
       "       'fBodyBodyGyroJerkMag-skewness()', 'fBodyBodyGyroJerkMag-kurtosis()',\n",
       "       'angle(tBodyAccMean,gravity)', 'angle(tBodyAccJerkMean),gravityMean)',\n",
       "       'angle(tBodyGyroMean,gravityMean)',\n",
       "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
       "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'Activity'],\n",
       "      dtype='object', length=562)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "#data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angle(tBodyGyroJerkMean,gravityMean)    float64\n",
       "angle(X,gravityMean)                    float64\n",
       "angle(Y,gravityMean)                    float64\n",
       "angle(Z,gravityMean)                    float64\n",
       "Activity                                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data are all scaled from -1 (minimum) to 1.0 (maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>0.310155</td>\n",
       "      <td>-0.053391</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>-0.140589</td>\n",
       "      <td>-0.215088</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>-0.148775</td>\n",
       "      <td>-0.232057</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074472</td>\n",
       "      <td>-0.376278</td>\n",
       "      <td>-0.750809</td>\n",
       "      <td>-0.337422</td>\n",
       "      <td>0.346295</td>\n",
       "      <td>0.884904</td>\n",
       "      <td>-0.698885</td>\n",
       "      <td>-0.651732</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10295</th>\n",
       "      <td>0.363385</td>\n",
       "      <td>-0.039214</td>\n",
       "      <td>-0.105915</td>\n",
       "      <td>-0.305388</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>-0.196373</td>\n",
       "      <td>-0.373540</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>-0.270237</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101859</td>\n",
       "      <td>-0.320418</td>\n",
       "      <td>-0.700274</td>\n",
       "      <td>-0.736701</td>\n",
       "      <td>-0.372889</td>\n",
       "      <td>-0.657421</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>-0.655181</td>\n",
       "      <td>0.273578</td>\n",
       "      <td>0.182412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10296</th>\n",
       "      <td>0.349966</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>-0.115788</td>\n",
       "      <td>-0.329638</td>\n",
       "      <td>-0.042143</td>\n",
       "      <td>-0.250181</td>\n",
       "      <td>-0.388017</td>\n",
       "      <td>-0.133257</td>\n",
       "      <td>-0.347029</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066249</td>\n",
       "      <td>-0.118854</td>\n",
       "      <td>-0.467179</td>\n",
       "      <td>-0.181560</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.696663</td>\n",
       "      <td>0.363139</td>\n",
       "      <td>-0.655357</td>\n",
       "      <td>0.274479</td>\n",
       "      <td>0.181184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>0.237594</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>-0.096499</td>\n",
       "      <td>-0.323114</td>\n",
       "      <td>-0.229775</td>\n",
       "      <td>-0.207574</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>-0.289477</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046467</td>\n",
       "      <td>-0.205445</td>\n",
       "      <td>-0.617737</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>-0.819188</td>\n",
       "      <td>0.929294</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>-0.659719</td>\n",
       "      <td>0.264782</td>\n",
       "      <td>0.187563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>0.153627</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>-0.137018</td>\n",
       "      <td>-0.330046</td>\n",
       "      <td>-0.195253</td>\n",
       "      <td>-0.164339</td>\n",
       "      <td>-0.430974</td>\n",
       "      <td>-0.218295</td>\n",
       "      <td>-0.229933</td>\n",
       "      <td>-0.111527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010386</td>\n",
       "      <td>-0.072237</td>\n",
       "      <td>-0.436940</td>\n",
       "      <td>0.598808</td>\n",
       "      <td>-0.287951</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>-0.024965</td>\n",
       "      <td>-0.660080</td>\n",
       "      <td>0.263936</td>\n",
       "      <td>0.188103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10299 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "0               0.288585          -0.020294          -0.132905   \n",
       "1               0.278419          -0.016411          -0.123520   \n",
       "2               0.279653          -0.019467          -0.113462   \n",
       "3               0.279174          -0.026201          -0.123283   \n",
       "4               0.276629          -0.016570          -0.115362   \n",
       "...                  ...                ...                ...   \n",
       "10294           0.310155          -0.053391          -0.099109   \n",
       "10295           0.363385          -0.039214          -0.105915   \n",
       "10296           0.349966           0.030077          -0.115788   \n",
       "10297           0.237594           0.018467          -0.096499   \n",
       "10298           0.153627          -0.018437          -0.137018   \n",
       "\n",
       "       tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "0             -0.995279         -0.983111         -0.913526         -0.995112   \n",
       "1             -0.998245         -0.975300         -0.960322         -0.998807   \n",
       "2             -0.995380         -0.967187         -0.978944         -0.996520   \n",
       "3             -0.996091         -0.983403         -0.990675         -0.997099   \n",
       "4             -0.998139         -0.980817         -0.990482         -0.998321   \n",
       "...                 ...               ...               ...               ...   \n",
       "10294         -0.287866         -0.140589         -0.215088         -0.356083   \n",
       "10295         -0.305388          0.028148         -0.196373         -0.373540   \n",
       "10296         -0.329638         -0.042143         -0.250181         -0.388017   \n",
       "10297         -0.323114         -0.229775         -0.207574         -0.392380   \n",
       "10298         -0.330046         -0.195253         -0.164339         -0.430974   \n",
       "\n",
       "       tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "0             -0.983185         -0.923527         -0.934724  ...   \n",
       "1             -0.974914         -0.957686         -0.943068  ...   \n",
       "2             -0.963668         -0.977469         -0.938692  ...   \n",
       "3             -0.982750         -0.989302         -0.938692  ...   \n",
       "4             -0.979672         -0.990441         -0.942469  ...   \n",
       "...                 ...               ...               ...  ...   \n",
       "10294         -0.148775         -0.232057          0.185361  ...   \n",
       "10295         -0.030036         -0.270237          0.185361  ...   \n",
       "10296         -0.133257         -0.347029          0.007471  ...   \n",
       "10297         -0.279610         -0.289477          0.007471  ...   \n",
       "10298         -0.218295         -0.229933         -0.111527  ...   \n",
       "\n",
       "       fBodyBodyGyroJerkMag-meanFreq()  fBodyBodyGyroJerkMag-skewness()  \\\n",
       "0                            -0.074323                        -0.298676   \n",
       "1                             0.158075                        -0.595051   \n",
       "2                             0.414503                        -0.390748   \n",
       "3                             0.404573                        -0.117290   \n",
       "4                             0.087753                        -0.351471   \n",
       "...                                ...                              ...   \n",
       "10294                         0.074472                        -0.376278   \n",
       "10295                         0.101859                        -0.320418   \n",
       "10296                        -0.066249                        -0.118854   \n",
       "10297                        -0.046467                        -0.205445   \n",
       "10298                        -0.010386                        -0.072237   \n",
       "\n",
       "       fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                            -0.710304                    -0.112754   \n",
       "1                            -0.861499                     0.053477   \n",
       "2                            -0.760104                    -0.118559   \n",
       "3                            -0.482845                    -0.036788   \n",
       "4                            -0.699205                     0.123320   \n",
       "...                                ...                          ...   \n",
       "10294                        -0.750809                    -0.337422   \n",
       "10295                        -0.700274                    -0.736701   \n",
       "10296                        -0.467179                    -0.181560   \n",
       "10297                        -0.617737                     0.444558   \n",
       "10298                        -0.436940                     0.598808   \n",
       "\n",
       "       angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                                  0.030400                         -0.464761   \n",
       "1                                 -0.007435                         -0.732626   \n",
       "2                                  0.177899                          0.100699   \n",
       "3                                 -0.012892                          0.640011   \n",
       "4                                  0.122542                          0.693578   \n",
       "...                                     ...                               ...   \n",
       "10294                              0.346295                          0.884904   \n",
       "10295                             -0.372889                         -0.657421   \n",
       "10296                              0.088574                          0.696663   \n",
       "10297                             -0.819188                          0.929294   \n",
       "10298                             -0.287951                          0.876030   \n",
       "\n",
       "       angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                                 -0.018446             -0.841247   \n",
       "1                                  0.703511             -0.844788   \n",
       "2                                  0.808529             -0.848933   \n",
       "3                                 -0.485366             -0.848649   \n",
       "4                                 -0.615971             -0.847865   \n",
       "...                                     ...                   ...   \n",
       "10294                             -0.698885             -0.651732   \n",
       "10295                              0.322549             -0.655181   \n",
       "10296                              0.363139             -0.655357   \n",
       "10297                             -0.008398             -0.659719   \n",
       "10298                             -0.024965             -0.660080   \n",
       "\n",
       "       angle(Y,gravityMean)  angle(Z,gravityMean)  \n",
       "0                  0.179941             -0.058627  \n",
       "1                  0.180289             -0.054317  \n",
       "2                  0.180637             -0.049118  \n",
       "3                  0.181935             -0.047663  \n",
       "4                  0.185151             -0.043892  \n",
       "...                     ...                   ...  \n",
       "10294              0.274627              0.184784  \n",
       "10295              0.273578              0.182412  \n",
       "10296              0.274479              0.181184  \n",
       "10297              0.264782              0.187563  \n",
       "10298              0.263936              0.188103  \n",
       "\n",
       "[10299 rows x 561 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1]\n",
    "#data.iloc[:, :-1].min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    561\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1].max().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the breakdown of activities--they are relatively balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAYING                1944\n",
       "STANDING              1906\n",
       "SITTING               1777\n",
       "WALKING               1722\n",
       "WALKING_UPSTAIRS      1544\n",
       "WALKING_DOWNSTAIRS    1406\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n",
    "Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6235     5\n",
       "9752     2\n",
       "10137    2\n",
       "1343     1\n",
       "7168     2\n",
       "Name: Activity, dtype: int32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Activity'] = le.fit_transform(data.Activity)\n",
    "data['Activity'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Calculate the correlations between the dependent variables.\n",
    "* Create a histogram of the correlation values\n",
    "* Identify those that are most correlated (either positively or negatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation values\n",
    "feature_cols = data.columns[:-1]\n",
    "corr_values = data[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.NaN\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values.stack().to_frame().reset_index().rename(columns={'level_0':'feature1','level_1':'feature2',0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of the absolute value correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50)\n",
    "\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n",
    "* Regardless of methods used to split the data, compare the ratio of classes in both the train and test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1,test_size=0.3, random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'Activity']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n",
    "* Using cross validation to determine the hyperparameters, fit models using L1, and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard logistic regression\n",
    "lr = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L1 regularized logistic regression\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try with different solvers like ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’ and give your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the coefficients into a dataframe\n",
    "coefficients = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    coeffs = mod.coef_\n",
    "    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n",
    "                                 labels=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n",
    "    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n",
    "\n",
    "coefficients = pd.concat(coefficients, axis=1)\n",
    "\n",
    "coefficients.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axList = plt.subplots(nrows=3, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "\n",
    "for ax in enumerate(axList):\n",
    "    loc = ax[0]\n",
    "    ax = ax[1]\n",
    "    \n",
    "    data = coefficients.xs(loc, level=1, axis=1)\n",
    "    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n",
    "    \n",
    "    if ax is axList[0]:\n",
    "        ax.legend(loc=4)\n",
    "        \n",
    "    ax.set(title='Coefficient Set '+str(loc))\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Also store the probability for the predicted class for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class and the probability for each\n",
    "\n",
    "y_pred = list()\n",
    "y_prob = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n",
    "    \n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "y_prob = pd.concat(y_prob, axis=1)\n",
    "\n",
    "y_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* fscore\n",
    "* confusion matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Preciision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n",
    "    \n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "    \n",
    "    # ROC-AUC scores can be calculated by binarizing the data\n",
    "    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n",
    "              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n",
    "              average='weighted')\n",
    "    \n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "    \n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                              'fscore':fscore, 'accuracy':accuracy,\n",
    "                              'auc':auc}, \n",
    "                             name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Display or plot the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axList = plt.subplots(nrows=2, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "axList[-1].axis('off')\n",
    "\n",
    "for ax,lab in zip(axList[:-1], coeff_labels):\n",
    "    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n",
    "    ax.set(title=lab);\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 9\n",
    " Identify highly correlated columns and drop those columns before building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#threshold with .7\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.7 * (1 - .7)))\n",
    "\n",
    "data2 = pd.concat([X_train,X_test])\n",
    "data_new = pd.DataFrame(sel.fit_transform(data2))\n",
    "\n",
    "\n",
    "data_y = pd.concat([y_train,y_test])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_new,X_test_new = train_test_split(data_new)\n",
    "Y_new,Y_test_new = train_test_split(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Repeat Model building with new training data after removing higly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try standard, L1 and L2 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try with different solvers like ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’ and give your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine all the coefficients into a dataframe for comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Also store the probability for the predicted class for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the class and the probability for each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* fscore\n",
    "* confusion matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the error metrics as listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "Display or plot the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform a comparison of the outputs between Question 7 and 12 and give your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform a comparison of the outputs between Question 8 and 13 and give your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
