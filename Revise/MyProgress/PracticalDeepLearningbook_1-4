1.Landscape
2.Image Classification
3.Transfer Learning
4.Reverse Image Search Engine

Install keras-vis using pip, run our script
Heatmaps are a great way to visually detect bias in the data.
Be vigilant at the outset of potential bias that can pollute our model’s learning.

Labeling companies like Appen and Scale AI are already valued at more than a billion dollars each.

NVIDIA GeForce RTX 2080 Ti.
Microsoft Bing runs the majority of its AI on FPGAs.

Types of Bias:
1.Selection Bias - The dataset is not representative of the distribution of the real-world
					problem and is skewed toward a subset of categories.
2.Implicit Bias - This type of bias creeps in because of implicit assumptions that we all
				  make when we see something
3.Reporting Bias - loudest voices in the room are the most extreme ones
					and dominate the conversation
4.In-group/Out-group Bias - Statue of Liberty and give it tags like “America” or “United States,”

Explainable AI-wherein the model would be able to not just provide predictions but also account for the factors that caused it to
make a certain prediction, and reveal areas of limitations.

One-pixel attacks in CNN - Essentially, the objective is to find and modify a single pixel in an image to
make a CNN predict something entirely different.
This is particularly relevant for selfdriving cars

Federated Learning for Privacy

Lambda Stack
https://lambdalabs.com/lambda-stack-deep-learning-software

Keras provides the ImageDataGenerator function that augments the data while it is being loaded from the directory.
example augmentations generated by the imgaug library for a sample image.

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range=20, width_shift_range=0.2,
				height_shift_range=0.2,
				zoom_range=0.2)
val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)	   >>>>we don’t want to augment our validation set

Sometimes knowing the label of a training image can be useful(8 instead of 6)

train_generator = train_datagen.flow_from_directory(
						TRAIN_DATA_DIR,
						target_size=(IMG_WIDTH, IMG_HEIGHT),
						batch_size=BATCH_SIZE,
						shuffle=True,
						seed=12345,
						class_mode='categorical')

model.fit_generator(train_generator,validation_data = validation_generator,...)

Reverse Image Search Engine
============================
Reverse image search (or as it is more technically known, instance retrieval) enables developers and researchers to build scenarios
beyond simple keyword search. From discovering visually similar objects on Pinterest to recommending similar songs on Spotify to
camera-based product search on Amazon, a similar class of technology under the hood is used. 
Sites like TinEye alert photographers on copyright infringement when their photographs are posted without consent on the internet. 
Even face recognition in several security systems uses a similar concept to ascertain the identity of the person.

Caltech101 and Caltech256 datasets
Performing feature extraction and similarity search

Similarity??
1. Compare hashes of the patches -- identification of plagiarism in photographs
2. Compare RGB histogram -- image deduplication software aimed at finding bursts of photographs on your hard disk

3. A more robust traditional computer vision-based approach is to find
visual features near edges using algorithms like 
Scale-Invariant Feature Transform (SIFT), 
Speeded Up Robust Features (SURF),
Oriented FAST and Rotated BRIEF (ORB)
These don't work well with deformable objects

4. Find category and similar images in the category. This does not work well too

CNNs to the rescue.
feature vectors of a thousand dimensions
The feature vectors (also called embeddings or bottleneck features) are essentially a
collection of a few thousand floating-point values.

features = extract_features('../../sample_images/cat.jpg', model)
Extract features for the entire dataset.

feature_list = []
for i in tqdm_notebook(range(len(filenames))):
	feature_list.append(extract_features(filenames[i], model))

Write these features to a pickle file so that we can use them in the future without having to recalculate them.
On a CPU, this should take under an hour. On a GPU, only a few minutes.

Extracting features part is completed..Now comes similarity search.

Similarity Search - Use KNNs to build a model.

Visualizing Image Clusters with t-SNE
======================================
It’s not possible to plot a 2,048-dimension vector (the feature-length) in two dimensions (the paper). The t-distributed
stochastic neighbor embedding (t-SNE) algorithm reduces the high-dimensional feature vector to 2D.(Use Projector in TensorBoard to view T-SNE)

t-SNE is difficult to scale to large datasets, so it is a good	idea to reduce the dimensionality using Principal Component
Analysis (PCA) and then call t-SNE.

In order to scale to larger dimensions, use Uniform Manifold Approximation and Projection (UMAP).

We have seen t-SNE visualization.

2D clusters are great, but visualizing them in 3D would look stellar - Use TensorFlow Embedding projector

Improving the Speed of Similarity Search
========================================
two strategies: 
1.either reduce the feature-length(Using Pooling or PCA) 
2.use a better algorithm to search among the features.

1st Point:
==========
Model 		# features before pooling 		# features after pooling
ResNet-50 	[1,1,1,2048] = 2048 			2048
InceptionV3 [1,5,5,2048] = 51200 			2048
MobileNet 	[1,7,7,1024] = 50176 			1024

model = InceptionV3(weights='imagenet', include_top=False, input_shape = (224,224,3), pooling='max') >>>Included max pool here

Apart from just the size, this is an even bigger improvement for big data scenarios, for which the data can be loaded into RAM all at once
instead of periodically loading parts of it - Thus increasing speed

Reducing Feature-Length with PCA:
PCA can also tell us the relative importance of each feature. The very first dimension has the most variance and the variance keeps
on decreasing as we go on.
How many n_components??? Plot (Variance vs PCA dimension count)
After the first 100 dimensions, the additional dimensions don’t add much variance
Find a good balance b/w n_components and accuracy

2nd Point:
==========
Use Approximate nearest-neighbor (ANN) instead of KNNs

There are several approximate nearest-neighbor (ANN) libraries
1.Spotify’s Annoy, 
2.FLANN,
3.Facebook’s Faiss, 
4.Yahoo’s NGT
5.NMSLIB. 

Benchmarking on 19 libraries on large public datasets:(queries/sec at some recall like 0.8)
We’ll pick the comparisons on a dataset of feature embeddings representing words (instead of images) called GloVe. This 350 MB dataset
consists of 400,000 feature vectors representing words in 100 dimensions.

Our brute-force search performs under 1 query per second. 
At the fastest, some of these libraries (like NGT) can return north of 15,000 results per second

Annoy(Approximate Nearest Neighbors Oh Yeah) 
Its a C++ library with Python bindings for searching NNs.
We build a search index with two hyperparameters: the number of dimensions of the dataset
and the number of trees(stress on trees).

NGT - Yahoo's , leads most benchmarks. Used on large datasets.
Faiss - Facebooks, It’s one of the fastest known implementations of ANN search on GPU.

Improving Accuracy with Fine Tuning with Fully Connected Layers
================================================================
Get worst perfoming classes(least accurate) before fine-tuning.

What is a good metric to check whether you are indeed getting similar images?
Painful option 1 - manually score through the entire dataset one image at a time
Happier option 2 - Simply calculate accuracy. That is, for an image belonging to
category X, are the similar images belonging to the same
category? We will refer to this similarity accuracy.

Visualized these worst performing classes on t-sne, they are all over the place.

Next check these classes after fine-tuning, 
t-SNE will cluster these classes properly.

The one limitation for fine tuning is the requirement of labeled data, which is not always present.
There’s a small unconventional training trick involved, though.

Fine Tuning Without Fully Connected Layers
===========================================
We fine tuned our model in Chapter 3 for classification.
(Convolutional layers, which end up generating the feature vectors
+Fully connected layers
+The final classifier layer)

The fully connected layers do a lot of the heavy lifting to get maximum classification accuracy. As a result, the majority of the
network that generates the feature vectors will change insignificantly. Thus, the feature vectors, despite fine tuning, will
show little change.

We fine tune our model for similarity search
(Convolutional layers, which end up generating the feature vectors
+Fully connected layers			>>>>>> This layer is removed
+The final classifier layer)	>>>>>> Pop this after fine-tuning

After fine-tuning , pop the last layer

Siamese Networks for One-Shot Face Verification
===============================================
By feeding a pair of images with the associated label, similar or dissimilar, and training the network end to end, the
embeddings begin to capture the fine-grained representation of the inputs. 
This approach of directly optimizing for the distance metric is called metric learning.

Contrastive Loss
Triplet Loss - Gives better results

Use Cases:
==========
Flickr adopted an ANN algorithm called Locally Optimized Product Quantization (LOPQ)
Open sourced in Python as well as Spark implementations

Pinterest in its features called Similar Pins and Related Pins. Not every pin on Pinterest has associated metadata, which makes
recommendation a difficult cold-start problem due to lack of context. Pinterest developers solved this cold-start problem by
using the visual features for generating the related pins. Additionally, Pinterest implements an incremental fingerprinting
service that generates new digital signatures if either a new image is uploaded.

Celebrity Doppelgangers look for the nearest neighbor among celebrities

Spotify
--------
Usually, collaborative filtering techniques, which are employed for recommending content like
movies on Netflix, are content agnostic. 
This presents a problem for new and not yet popular content because users will keep getting recommendations for
existing popular content. This is also referred to as the aforementioned cold-start problem.
Deep content-based music recommendation::
We can create feature vectors out of music using MFCC features (Mel Frequency Cepstral Coefficients), which in turn generates a 
2D spectrogram that can be thought of as an image and can be used to generate features. Songs are divided into three-second fragments, and their spectrograms are used to generate features. These features are then averaged together to represent the complete song.

Image Captioning
----------------
MS COCO - more than 300,000 images along with object categories, sentence descriptions, visual question-answer pairs,
and object segmentations.

A simple nearest-neighbor search could yield state-of-the-art results. 
1. For a given image, find similar images based on similarity of the embeddings. 
2. Then, note the common words in the captions of the similar images, and print the caption containing the most common words. 
This exposed a critical bias in the dataset - Most Girraffe's stand on Grass.. So Grass will be common in these words






