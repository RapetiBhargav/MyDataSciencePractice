Training and Deploying TensorFlow Models at Scale:
==================================================
You need to regularly retrain your model on fresh data and 
Push the updated version to production. 
Handle model versioning,
Gracefully transition from one model to the next, possibly roll back 
Run multiple different models in parallel to perform A/B experiments
Many queries per second (QPS): scale up to support the load.

TF Serving, either on your own hardware infrastructure or via a cloud service such as Google Cloud AI Platform.

If your product needs to adapt to changes quickly, then a long training time can be a showstopper

One way to speed up training is to use hardware accelerators such as GPUs or TPUs. To go even
faster, you can train a model across multiple machines, each equipped with multiple
hardware accelerators. TensorFlow’s simple yet powerful Distribution Strategies API makes this easy.

What we discuss:
1.How to deploy models, first to TF Serving, then to Google Cloud AI Platform. 
2.We will also take a quick look at deploying models to mobile apps, embedded devices, and web apps. 
3.Lastly, we will discuss how to speed up computations using GPUs and how to train models across multiple devices and servers
using the Distribution Strategies API.

Serving a TensorFlow Model
===========================
You could create your own microservice using any technology you want (e.g., using the Flask library), but
why reinvent the wheel when you can just use TF Serving?

Using TensorFlow Serving
=========================
TF Serving is a very efficient, battle-tested model server that’s written in C++.
Watch to automatically deploy the latest versions

To deploy model to TF Serving : Export this model to TensorFlow’s SavedModel format.
Simple tf.saved_model.save() function to export models to the SavedModel format.

Just use the model’s save() method (model.save(model_path)): as long as the file’s extension is not .h5, the model will be saved using the
SavedModel format instead of the HDF5 format.

Dynamic models need to be served using other tools (e.g., Flask).

saved_model_cli command-line tool to inspect SavedModels

A SavedModel contains one or more metagraphs. A metagraph is a computation
graph plus some function signature definitions.  Each metagraph is identified by a set of tags.

Installing TensorFlow Serving
==============================
Many ways to install TF Serving:
Using a Docker image(easier option)
Using the system’s package manager
Installing from source

Pull the TF Serving docker image.
docker pull tensorflow/serving
docker run

Makes the Docker engine forward the host’s TCP port 8500 to the container’s TCP port 8500.

Querying TF Serving through the REST API and gRPC
=================================================
It works well when the input and output data are not too large.
When transferring large amounts of data, it is much better to use the gRPC API

The gRPC API expects a serialized PredictRequest protocol buffer as input, and it
outputs a serialized PredictResponse protocol buffer. 
These protobufs are part of the tensorflow-serving-api library

Input : Convert numpy/tensor to protocol buffer
Output: Convert protocol buffer to tensor.

--enable_batching option upon startup.

Deploying a new model version
==============================
Just place the saved model in the location TF Serving will automatically pick it up

If you expect to get many queries per second, you will want to deploy TF Serving on
multiple servers and load-balance the queries. This will require
deploying and managing many TF Serving containers across these servers. One way
to handle that is to use a tool such as Kubernetes, which is an open source system for
simplifying container orchestration across many servers.
You will want to use virtual machines on a cloud platform such as Amazon AWS, Microsoft Azure, Google
Cloud Platform, IBM Cloud, Alibaba Cloud, Oracle Cloud, or some other Platformas-
a-Service (PaaS). Managing these can be a full time job.
Luckily GCP!!! or AWS Sagemaker

Creating a Prediction Service on GCP AI Platform
=================================================
1. Create project(it has a unique ID and number)
2. GS-->> upload your model in google storage
3. In AI Platform, create model and refer to this GS location.
4. Set the minimum number of TF Serving containers
to 1 when creating the model version.(since it needs to be started)

Using the Prediction Service
=============================
IAM & admin → Service accounts, then click Create Service Account, enter some details..download the private key(JSON file)

Google API Client Library
--------------------------
This is a fairly thin layer on top of OAuth 2.0 (for the authentication) and REST.
You can use it with all GCP services, including AI Platform.

Google Cloud Client Libraries
------------------------------
These are a bit more high-level: each one is dedicated to a particular service, such
as GCS,Google BigQuery, Google Cloud Natural Language, and Google Cloud Vision.
When a client library is available for a given service, it is recommended to use it rather 
than the Google API Client Library, as it implements all the best practices and will often use gRPC rather than REST, for
better performance.

No client library for AI Platform, so we will use the Google API Client Library

A/B testing : For testing a new version on a small group of users before releasing it widely(this is called a canary).
You can also get detailed logs and metrics using Google Stackdriver.

Deploying a Model to a Mobile or Embedded Device
=================================================
TFLite’s model converter can take a SavedModel and compress it to a much lighter format based on FlatBuffers. This is an efficient crossplatform serialization library (a bit like protocol buffers) initially created by Google for gaming.
It is designed so you can load FlatBuffers straight to RAM without any preprocessing.

Quantizing the model weights down to fixed-point, 8-bit integers rather than 32-bit floats:
The simplest approach is called post-training quantization: it just quantizes the weights after training, 
using a fairly basic but efficient symmetrical quantization technique.

Maps the floating-point range –m to +m [m is maximum absolute value] to the fixed-point (integer) range –127 to +127.

To perform this post-training quantization, simply add OPTIMIZE_FOR_SIZE to the list
of converter optimizations before calling the convert() method.

converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]

At runtime the quantized weights gets converted back to floats.(but no accuracy loss)

Moreover, some neural network accelerator devices (such as the Edge TPU) can only process integers, 
so full quantization of both weights(8-bit integers) and activations(32 bit integers) is compulsory. 
This can be done post-training; it requires a calibration step to find the max absolute value of the activations, >>Posttraining you will need a calibration step
so you need to provide a representative sample of training data to TFLite (it does not
need to be huge), and it will process the data through the model and measure the
activation statistics required for quantization (this step is typically fast).

If the accuracy drop is too severe, then you may need to use quantization-aware training.

Tensorflow in Browser:
======================
You can export your model to a special format that can be loaded by the TensorFlow.js JavaScript library. This library can then use your model
to make predictions directly in the user’s browser. The TensorFlow.js project includes a tensorflowjs_converter tool that can convert a TensorFlow SavedModel or a Keras model file to the TensorFlow.js Layers format: this is a directory containing a set of sharded weight files in binary format and a model.json file that describes the model’s architecture and links to the weight files.

import * as tf from '@tensorflow/tfjs';
const model = await tf.loadLayersModel('https://example.com/tfjs/model.json');

Federated Learning

Using GPUs to Speed Up Computations
====================================
Get a major performance boost simply by adding GPU cards to a single machine. 
Due to the extra delay imposed by network communications in a distributed setup, using a single powerful GPU
is often preferable to using multiple slower GPUs.

2 options for this:
1. purchase your own GPU(s), or 
2. you can use GPU-equipped virtual machines on the cloud.

https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/

TensorFlow only supports Nvidia cards with CUDA Compute Capability 3.5+ (as well as Google’s TPUs, of course)

If you go for an Nvidia GPU card, you will need to install the appropriate Nvidia drivers and several Nvidia libraries. These include the Compute Unified Device Architecture library (CUDA), which allows developers to use CUDA-enabled GPUs for all sorts of computations (not just graphics acceleration), and the CUDA Deep Neural Network library (cuDNN), a GPU-accelerated library of primitives for DNNs.
It is part of Nvidia’s Deep Learning SDK

TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs

nvidia-smi command to check that CUDA is properly installed.

Since installing every required library correctly is a bit long and tricky (and all hell breaks loose if you do not install the correct library versions), TensorFlow provides a Docker image with everything you need inside. However, in order for the Docker container
to have access to the GPU, you will still need to install the Nvidia drivers on the host machine.

tf.test.is_gpu_available()
tf.test.gpu_device_name()
tf.config.experimental.list_physical_devices(device_type='GPU')

*****************************************************************************************************************************************
Abhishek Thakur's Training a deep learning model using Docker.:
####################################################################
1.In dockerFile we are using base image as Ubuntu.It does not come with Cuda and Cudnn drivers
or you can do something else.(Use nvidia/cuda image from dockerhub)

Use below one as base image
10.1-cudnn7-runtime-ubuntu18.04
docker pull nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04

18.04 because the drivers we install here are 18.04.

2.If you want to train a deep learning model using NVidia drivers, Install the docker runtime that Nvidia provides.
Build and run Docker containers leveraging NVIDIA GPUs

Ubuntu 16.04/18.04/20.04, Debian Jessie/Stretch/Buster
Run these in terminal window
============================= 
# Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit				>>>>>>>>>Basically install the Nvidia Container Toolkit
sudo systemctl restart docker

Build the image,and container
docker build -f DockerFile -t melanoma:train .

It will successfully tag melanoma:train.

Now lets train the model:
docker run --gpus 1 -v /home/abhishek/workspace/melanoma_data:/home/abhishek/data -ti melanoma:train python3 main.py

Here we are mounting the volume for training data (local machine:docker container)

Docker has to share memory from Host machine.
To avoid this make the number of workers to 0 in your data loaders

Final command to run for successful training.
docker run --gpus 1 --ipc=host -v /home/abhishek/workspace/melanoma_data:/home/abhishek/data -ti melanoma:train python3 main.py
*****************************************************************************************************************************************

Using a GPU-Equipped Virtual Machine:
=====================================
Preconfigured with all the drivers and libraries you need (including TensorFlow).

IAM & admin → Quotas-->search for “GPU,” and select the type of GPU you want (e.g., NVIDIA P4 GPUs).
Goto https://cloud.google.com/deep-learning-vm/ and create a Deep Learning VM
“Install NVIDIA GPU driver automatically on first startup.”
“Enable access to JupyterLab via URL instead of SSH”
AI Platform → Notebooks

Colaboratory
============
It runs on a free Google VM dedicated to that notebook, called a Colab Runtime - CPU, GPU or even TPUs
Automatically disconnect from the Colab Runtime if you leave it unattended for a while (~30 minutes).

Managing the GPU RAM
=====================
By default TensorFlow automatically grabs all the RAM in all available GPUs
If you have multiple GPU cards on your machine, a simple solution is to assign each
of them to a single process.

$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py (OR)
os.environ["CUDA_DEVICE_ORDER"] and os.environ["CUDA_VISIBLE_DEVICES"]

Virtual GPU device(Grab only 2BG of RAM)
for gpu in tf.config.experimental.list_physical_devices("GPU"):
	tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])

Tell TF to grab memory only when it needs it
for gpu in tf.config.experimental.list_physical_devices("GPU"):
	tf.config.experimental.set_memory_growth(gpu, True)

The following code splits the first GPU into two virtual devices, with 2 GiB of RAM each
physical_gpus = tf.config.experimental.list_physical_devices("GPU")
	tf.config.experimental.set_virtual_device_configuration(physical_gpus[0],
	[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),
	tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])

Now let’s see how TensorFlow decides which devices it should place variables and execute operations on.

Placing Operations and Variables on Devices
============================================
tf.keras and tf.data generally do a good job of placing operations and variables(e.g., heavy computations on the GPU, and data preprocessing
on the CPU)

****Important point
Generally want to place the data preprocessing operations
on the CPU, and place the neural network operations on the GPUs.

If a variable is not needed in the next few training steps, it should probably be placed on the CPU 
(e.g., datasets generally belong on the CPU).

By default, all variables and all operations will be placed on the first GPU(If they have a GPU kernel)

A tensor or variable’s device attribute tells you which device it was placed on:
a = tf.Variable(42.0)								>>>>>>>>>>>float has a GPU kernel, integer does'nt have a GPU kernel.
>>> a.device
'/job:localhost/replica:0/task:0/device:GPU:0'

with tf.device("/cpu:0"):				>>>>>If you want to place it on specific device
	c = tf.Variable(42.0)

The CPU is always treated as a single device (/cpu:0), even if your machine has multiple CPU cores.

tf.config.set_soft_device_placement(True)     >>>>fall back to the CPU;if GPU is not available

Parallel Execution Across Multiple Devices
==========================================
Operations in CPU:(single-threaded and multi-threaded kernels)
------------------
Operations in the CPU’s evaluation queue are dispatched to a thread pool called the inter-op thread pool.
Some operations have multithreaded CPU kernels:
These kernels split their tasks into multiple suboperations, which are placed in
another evaluation queue and dispatched to a second thread pool called the intra-op thread pool

Operations in GPU:(single-threaded and multi-threaded kernels)
------------------
Operations in a GPU’s evaluation queue are evaluated sequentially. 
Most operations have multithreaded GPU kernels.
There is no need for an inter-op thread pool in GPUs: each operation already floods most GPU threads.

tf.config.threading.set_inter_op_parallelism_threads()
tf.config.threading.set_intra_op_parallelism_threads()
This is useful if you want do not want TensorFlow to use all the CPU cores or if you want it to be single-threaded.

If your model takes two images as input and processes them using two CNNs
before joining their outputs(ensemble), then it will probably run much faster if you place
each CNN on a different GPU.

Till now we saw how train a several models in parallel each on its own GPU...(CUDA_DEVICE_ORDER and CUDA_VISIBLE_DEVICES)
What if we want to train a single model on multiple devices????

Training Models Across Multiple Devices
=======================================
Model Parallelism - model is split across the devices
-----------------
It really depends on the architecture of your neural network.
For a DNN -- Vertical or Horizonal split, not great
For a CNN -- contain layers that are only partially connected to the lower layers, so Vertical split is fairly good
For a RNN -- Each layer can run on a different GPU - Horizonal split. But in practice a regular stack of LSTM layers running on a single GPU actually runs much faster.

Cross Device communication, need tuning, GPUs on same device etc etc-Very complex
Lets look at a simpler option - Data Parallelism

Data Parallelism - model is replicated across every device, and each replica is trained on a subset of the data.
----------------
Based on how you stores your model parameters (On each device or in a centralized machine - Like a CPU), we have 2 options
1.Data parallelism using the mirrored strategy
Compute the average gradients from all the GPUs(AllReduce algorithm) and distribute the result.
The mirrored strategy imposes synchronous weight updates across all GPUs.

2.Data parallelism with centralized parameters
Workers- GPUs, Parameter Servers-CPU
This centralized approach allows either synchronous or asynchronous updates.

Pros and Cons for each of these methods:
-----------------------------------------
Synchronous updates.
The parameters will be copied to every device almost at the same time.
This setup is generally described as having 18 replicas plus 2 spare replicas.(Donno how this is acheived)

Asynchronous updates.
With asynchronous updates, whenever a replica has finished computing the gradients, it immediately uses them 
to update the model parameters. There is no aggregation.
There is no guarantee that the computed gradients will still be pointing in the right direction. 
When gradients are severely out-of-date, they are called stale gradients:they can slow down convergence.

A few ways you can reduce the effect of stale gradients:
• Reduce the learning rate.
• Adjust the mini-batch size.
• Start the first few epochs using just one replica (this is called the warmup phase).
  Stale gradients tend to be more damaging at the beginning of training, when gradients
  are typically large and the parameters have not settled into a valley

Overall:
Using synchronous updates with a few spare replicas was more efficient than using asynchronous updates, not only converging faster but also
producing a better model.

Bandwidth saturation:
There always comes a point where adding an extra GPU will not improve performance at all because the time spent moving the data into
and out of GPU RAM.
Beyond a few dozen GPUs for a dense model or few hundred GPUs for a sparse model, saturation kicks in.

Less severe for small models and for large sparse models.

Now let’s train a model across multiple GPUs!

Training at Scale Using the Distribution Strategies API
=======================================================
(How to train models at scale, starting with 
1.multiple GPUs on the same machine (or TPUs) and then moving on to 
2.multiple GPUs across multiple machines.)
We will discuss about point 2 in next section.

distribution = tf.distribute.MirroredStrategy()		>>>>or CentralStorageStrategy() or ParameterServerStrategy.. 
with distribution.scope():							>>>>Specify which GPUs to use(i.e compute_devices, parameter devices)
	mirrored_model = keras.models.Sequential([...])
	mirrored_model.compile([...])
batch_size = 100									>>>>Need to be divisible by No. of replicas
history = mirrored_model.fit()
predict() runs on multiple replicas

If you call the model’s save() method, it will be saved as a regular
model, not as a mirrored model with multiple replicas.

If you want to load a model and run it on all available devices, you must
call keras.models.load_model() within a distribution context:

AllReduce algorithms:
tf.distribute.NcclAllReduce class - Default for MirroredStrategy
tf.distribute.HierarchicalCopyAllReduce class
the tf.distribute.ReductionToOneDevice class

Training a Model on a TensorFlow Cluster
=========================================
To start a TensorFlow cluster, you must first specify it. This means defining each
task’s IP address, TCP port, and type(worker, parameter server etc).

cluster_spec = {
	"worker": [
		"machine-a.example.com:2222", # /job:worker/task:0
		"machine-b.example.com:2222" # /job:worker/task:1
	],
	"ps": ["machine-a.example.com:2221"] # /job:ps/task:0
}

Make sure to configure your firewall to allow interaction between these machines on ports.

TF_CONFIG ->> Has cluster_spec + Task

In general you want to define the TF_CONFIG environment variable outside of Python, so the code does not need to include the current
task’s type and index (this makes it possible to use the same code across all workers).

MultiWorkerMirroredStrategy
ParameterServerStrategy(You need to have TF_CONFIG appropriately)
TPUStrategy- For TPUs

AllReduce implementations for mirrored strategy:
Pass CollectiveCommunication.RING or CollectiveCommunication.NCCL (from tf.distribute.experimental) to the strategy’s constructor.
like tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.RING) etc...

Less hassle and less expensive to just use Cloud
Running Large Training Jobs on Google Cloud AI Platform
========================================================
Open GCloud shell or SDK 
Platform will take care of provisioning and configuring as many GPU VMs as you desire.
AI Platform will take care of setting TF_CONFIG for you on each VM.

$ gcloud ai-platform jobs submit training my_job_20190531_164700 \
--region asia-southeast1 \
--scale-tier PREMIUM_1 \					>>>>>>>20 workers, 11 parameter servers
--runtime-version 2.0 \
--python-version 3.5 \
--package-path /my_project/src/trainer \
--module-name trainer.task \
--staging-bucket gs://my-staging-bucket \
--job-dir gs://my-mnist-model-bucket/trained_model \
--
--my-extra-argument1 foo --my-extra-argument2 bar

Passing Training data to this job:
If you place the training data on GCS, you can create a tf.data.TextLineDataset or tf.data.TFRecordDataset to access it.
These datasets rely on the tf.io.gfile package to access files(supports both in local as well as in GCS)

Black Box Hyperparameter Tuning on AI Platform
===============================================
--config tuning.yaml along with the training job.
AI platform just monitors the --job-dir for any event file containing summaries for a metric named "accuracy"
training code simply has to use the TensorBoard() callback

AI platform can also be used to execute a model on huge data.

To do:
1.How to run Docker in Colab?
2.Hyperparameter tuning code in the end.