6.Maximizing speed: Handy Checklist
7.Tips and Tricks --> https://github.com/PracticalDL/Practical-Deep-Learning-Book/tree/master/code/chapter-7

GPU Starvation: The reality is that the CPU, memory, and storage are frequently the performance
bottlenecks, resulting in suboptimal utilization of the GPU. 
In other words, we want the GPU to be the bottleneck, not the other around.

In a deep learning pipeline, the CPU and GPU work in collaboration, passing data to each other.

Their collaboration is like a relay race, except one of the relay runners is an
Olympic athlete, waiting for a high school track runner to pass the
baton. The more time the GPU stays idle, the more wasted resources.

A large portion of this chapter is devoted to reducing the idle time of the GPU and the CPU

How do we know whether the GPU is starving? 
Two handy tools for this:
1.nvidia-smi
2.TensorFlow Profiler + TensorBoard

nvidia-smi
-----------
GPU utilization:Percent of time over the past second during which one or more kernels was executing on the GPU.
How do we continuously monitor these numbers?
We can refresh the utilization metrics every half a second with the watch command
$ watch -n .5 nvidia-smi
Numbers jumping around
Instead poll the GPU utilization every second and dump that into a file
$ nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits -fgpu_utilization.csv -l 1
Now, calculate the median GPU utilization from the file generated:
$ sort -n gpu_utilization.csv | grep -v '^0$' | datamash median 1

Datamash is a handy command-line tool that performs basic numeric,
textual, and statistical operations on textual data files.

TensorFlow Profiler(used by power users of TF) + TensorBoard
-------------------------------------------------------------
Could we get a deeper analysis? 
It turns out, for advanced users, TensorFlow provides a powerful set of tools.

Helps analyze and understand the training process at a much deeper level
Includes Trace Viewer, a feature that shows events in a timeline. It helps
investigate how resources are being used precisely at a given period of
time and spot inefficiencies.

Activating TensorBoard involves a simple callback function.

Users wanting even deeper access to their hardware utilization metrics, NVIDIA Nsight is a great tool.

Checklist:
==========
More efficient execution at each area of the deep learning pipeline step by step, including data preparation, data reading, data
augmentation, training, and, finally, inference.

Data Preparation
-----------------
“Store as TFRecords”
“Reduce Size of Input Data” - Perform resize and then save it to TFRecord files
“Use TensorFlow Datasets”

Data Reading
------------
“Use tf.data”
“Prefetch Data”
“Parallelize CPU Processing” - Use num_parallel_calls with map
“Parallelize I/O and Processing” - Use num_parallel_calls with Interleave
“Enable Nondeterministic Ordering” 
			When reading files in parallel, tf.data still attempts to produce their outputs in a fixed round-robin order. 
			The disadvantage is that we might encounter a “straggler” along the way (i.e., an operation that takes a lot longer than
			others, such as a slow file read, and holds up all other operations).
			Code before reading files:
			options = tf.data.Options()
			options.experimental_deterministic = False

“Cache Data” - To repeatedly avoid reading from the disk
			Dataset.cache() function allows us to make a copy of data either in memory or as a file on disk
			It is recommended to place cache() before any random augmentations and shuffling;
			Code:
			dataset = dataset.cache() # in-memory
			dataset = dataset.cache(filename='tmp.cache') # on-disk

“Turn on Experimental Optimizations”
			Benefit from being fused together multiple operations as one single operation.
			Filter fusion
			Map and filter fusion 
			Map fusion
			Code:
			options = tf.data.Options()
			options.experimental_optimization.filter_fusion = True
			dataset = dataset.with_options(options)
			
			After this you have optimized versions of map , filter
			dataset = dataset.filter(lambda x: x < 1000).filter(lambda x: x % 3 == 0)
			dataset = dataset.map(lambda x: x * x).filter(lambda x: x % 2 == 0)

“Autotune Parameter Values” - Just use tf.data.experimental.AUTOTUNE

Data Augmentation
------------------
“Use GPU for Augmentation”
OpenCV, Pillow, and the built-in Keras augmentation functionality are the most commonly used libraries in computer vision for working on
images. 
Limitation:
There’s one major limitation here, though. Their image processing is primarily CPU based (although you can compile OpenCV
to work with CUDA),
There are efforts underway to convert Keras image augmentation to be GPU accelerated.

***A few different GPU-bound options that we can explore:
1. tf.image provides some handy augmentation functions that we can
seamlessly plug into a tf.data pipeline.
Ex:
updated_image = tf.image.adjust_hue(image, delta = 0.2)
Limitation: Can't rotate by a arbitrary degree using tf.image

2. As another alternative to the tf.data pipeline, the NVIDIA Data Loading
Library (DALI) offers a fast data loading and preprocessing pipeline using GPU-bound
DALI works with multiple deep learning frameworks including TensorFlow, PyTorch, MXNet. See pic. NvidiaDALI
nvJPEG, a GPU-accelerated library for JPEG decoding.

Training
--------
“Use Automatic Mixed Precision”					>>>>>> Actually looks more/less like quantization aware training.
			In this method, we store the model in FP32 as a master copy and perform the forward/backward passes of training in
			FP16. After each training step is performed, the final update from that step is then scaled back up to FP32 before 
			it is applied to the master copy.
		
		To enable Automatic Mixed Precision during training
		os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'

“Use Larger Batch Size”
Large industrial training loads distributed across multiple nodes often use much
larger batch sizes with the help of a technique called Layer-wise Adaptive Rate Scaling (LARS).

“Use Multiples of Eight”
Using multiples of eight (or 16 in the case of INT8 operations)+ automatic mixed precision = bare minimum requirement to activate the Tensor Cores

“Find the Optimal Learning Rate”
The keras_lr_finder library

“Use tf.function”

“Overtrain, and Then Generalize”
How do we force a network to learn quickly and imperfectly? Make it overtrain on our data.3 strategies
	  “Use progressive sampling” - Train on Small data at first(10%) , then once the model converges(overtrain), train on the remaining 
	  “Use progressive augmentation” - Train on Small degree of augmentation at first(10%) , progressively increase the degree of Augmentation 
	  “Use progressive resizing” - Introduced by Jeremy Howard
	  “Use progressive subclassing” - Training on a subset of classes, then generalizing to all classes later.

“Install an Optimized Stack for the Hardware”
Optimization for CPU's:
-----------------------
Installing this general-purpose won't able to take advantage of the specific features offered by a particular hardware stack
This issue is one of the big reasons to avoid installing prebuilt binaries and instead opt for building packages from source.

Setting the configuration flags for the hardware before building the source code.
to enable support for AVX2 and SSE 4.2 instruction sets
$ bazel build -c opt --copt=-mavx2 --copt=-msse4.2
//tensorflow/tools/pip_package:build_pip_package
To check,
$ lscpu | grep Flags

But building from Source is slow...
Instead install highly optimized variant of TensorFlow(once you installed anaconda) - built by Intel on top of their Math Kernel Library for Deep Neural Networks (MKL-DNN).

# For Linux and Mac
$ conda install tensorflow
# For Windows
$ conda install tensorflow-mkl

Optimization for GPU's:
-----------------------
Just install tensorflow-gpu .. (Or Lambda stack)

“Optimize the Number of Parallel CPU Threads”
					Between operations
					#Example 1
					X = tf.multiply(A, B)
					Y = tf.multiply(C, D)
					Use tf.config.threading.set_inter_op_parallelism_threads(num_threads)  >>>>num_threads is number of cores(use lscpu)
					
					Per operations
					# Example 2
					X = tf.multiply(A, B)
					Y = tf.multiply(X, C)
					Use tf.config.threading.set_intra_op_parallelism_threads(num_threads)

“Use Better Hardware”
“Distribute Training”
					We need to configure the TF_CONFIG environment variable on every single host. 
					This requires setting up a JSON object that contains the IP addresses and ports of all other hosts in the cluster. 
					Manually managing this can be error prone, and this is where orchestration frameworks like Kubernetes really shine.
					Horovod(From Uber) was used earlier. We use the CollectiveAllReduceStrategy in TensorFlow now.

“Examine Industry Benchmarks”
					DAWNBench benchmarks time and cost to get a model to 93% Top-5 accuracy on ImageNet
					MLPerf - Much wider support, especially on the hardware side. 
					It runs challenges for both training and inference in two divisions: open and closed. 
					The closed division trains the same model with the same optimizers - So the raw hardware performance can be compared.
					The open division, on the other hand, allows using faster models and optimizers to allow for more rapid progress.
					Competitions for inference and training on low power devices.

Inference
---------
“Use an Efficient Model”
					Many machines would not be capable of serving a half gigabyte VGG-16 model
					MobileNet is the go-to model for efficient smartphone runtimes - by varying the hyperparameters of the MobileNet models
					like depth multiplier, the number of computations can be further reduced
					Or just find the optimal architecture using NAS.

“Quantize the Model” - FP32 to 8 bit integers
					To go below and still have a useful working model (like a 1-bit representation), we’d need to follow a
					special conversion process to convert them to binarized neural networks. 
					XNOR.ai, a deep learning startup, has famously been able to bring this technique to production. 
					The Microsoft Embedded Learning Library (ELL) similarly provides such tools, which have a lot of value for
					edge devices like the Raspberry Pi.
					Most inference frameworks provide a way to quantize - TFLite, TensorRT from NVIDIA etc.

“Prune the Model” - Close to zero, make it zero. Makes model's dense layer sparser - form of model compression
					Keras provides APIs to prune our model. This process can be done
					iteratively during training. Train a model normally or pick a pretrained
					model. Then, periodically prune the model and continue training.
					Have enough epochs between the periodic prunes.

Another way: Tencent’s PocketFlow tool, a one-line command that provides several other pruning strategies
https://github.com/Tencent/PocketFlow

“Use Fused Operations” - TFLite
“Enable GPU Persistence”
$ nvidia-persistenced --user {YOUR_USERNAME}
